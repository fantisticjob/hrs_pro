package com.hausontech.hrs.serviceImpl.etlProcess;  

import java.sql.SQLException;
import java.util.Date;

import org.pentaho.di.core.exception.KettleException;
import org.quartz.DisallowConcurrentExecution;
import org.quartz.Job;  
import org.quartz.JobDataMap;  
import org.quartz.JobExecutionContext;  
import org.quartz.JobExecutionException;
import org.quartz.PersistJobDataAfterExecution;

import com.hausontech.hrs.bean.etlProcess.JobInstanceBean;
import com.hausontech.hrs.daoImpl.IBaseDao2;
import com.hausontech.hrs.daoImpl.etlProcess.mapper.JobInstanceMapper;  
  
/** 
 * 任务执行类 
 * KETTLE初始化、运行等在service层所以job执行任务类只能在service层
 */
@PersistJobDataAfterExecution  
@DisallowConcurrentExecution
public class QuartzJob implements Job {  
    @Override  
    public void execute(JobExecutionContext content) throws JobExecutionException { 
        JobDataMap dataMap = content.getJobDetail().getJobDataMap();  
        JobInstanceBean dbJob = (JobInstanceBean) dataMap.get("reqBean");
        IBaseDao2 baseDao2=(IBaseDao2)dataMap.get("baseDao2");
        String jobName=(String) dataMap.get("jobName");
        JobInstanceMapper jobMapper =  (JobInstanceMapper) dataMap.get("jobMapper");
        long primaryKey=0;
		//Map<String, Object> map = new HashMap<>();
        KettleJobServiceImpl kettle=new KettleJobServiceImpl();
		try {
			kettle.initKettleEnv();
		} catch (KettleException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		dbJob.setStatus("processing");
		dbJob.setMessage("ETL job is running");
		String status = dbJob.getStatus();
		try {
			status = jobMapper.getStatus(null,status);
		} catch (SQLException e1) {
			// TODO Auto-generated catch block
			e1.printStackTrace();
		}
		dbJob.setStatus(status);
		primaryKey=baseDao2.getAutoGeneratedPrimaryKey("HRS_ETL_JOB_INSTANCE_S");
		dbJob.setLastJobInstanceId(primaryKey);
		dbJob.setStartTime(new Date());
		dbJob.setCreationDate(new Date());
		dbJob.setLastUpdateDate(new Date());
		dbJob.setCreatedBy("web");
		dbJob.setLastUpdatedBy("web");
		/*保存ETL任务执行实例*/
		try {
			jobMapper.saveEtlJobInstance(dbJob);
		} catch (SQLException e1) {
			// TODO Auto-generated catch block
			e1.printStackTrace();
		}
		long startTime = System.currentTimeMillis();
		try {
			kettle.runJobFromFileSystem(dbJob);
		} catch (Exception e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}	
		long ecalTime = System.currentTimeMillis() - startTime;
		dbJob.setElapsedTime((int)ecalTime);
		dbJob.setStatus("success");
		dbJob.setEndTime(new Date());
	    dbJob.setLastUpdateDate(new Date());
		status = dbJob.getStatus();
		String message = dbJob.getMessage();
		int length = message.length();
		int errorNumber = Integer.parseInt(message.substring(length - 9, length - 8));
		if (errorNumber > 0) {
			status = "failed";
		}
		try {
			status = jobMapper.getStatus(null,status);
		} catch (SQLException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		dbJob.setStatus(status);
		/*map.put("status", status);
		map.put("message",message);
		list.add(map);*/
		/*更新ETL任务执行实例*/
	    try {
			jobMapper.updateEtlJobInsRecord(dbJob);
		} catch (SQLException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
        System.out.println("并行-------执行任务名是="+jobName);  
    }  
}  